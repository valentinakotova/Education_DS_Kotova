{"metadata":{"colab":{"provenance":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":9560659,"sourceType":"datasetVersion","datasetId":5826158}],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"###ML1_4: Реализовать stemming, lemmatization & BoW на следующем датасете: https://cloud.mail.ru/public/Z4L3/vB8GcgTtK (Russian Toxic-abuse comments)","metadata":{}},{"cell_type":"code","source":"#Установка нужных пакетов\n!pip install --upgrade nltk gensim bokeh umap-learn\n\nimport itertools\nimport string\n\nimport numpy as np\nimport umap\nfrom nltk.tokenize import WordPunctTokenizer\n\nfrom matplotlib import pyplot as plt\n\nfrom IPython.display import clear_output","metadata":{"execution":{"iopub.status.busy":"2024-10-06T15:40:33.945322Z","iopub.execute_input":"2024-10-06T15:40:33.945767Z","iopub.status.idle":"2024-10-06T15:40:45.807149Z","shell.execute_reply.started":"2024-10-06T15:40:33.945727Z","shell.execute_reply":"2024-10-06T15:40:45.805468Z"},"trusted":true},"execution_count":132,"outputs":[{"name":"stdout","text":"Requirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (3.9.1)\nRequirement already satisfied: gensim in /opt/conda/lib/python3.10/site-packages (4.3.3)\nRequirement already satisfied: bokeh in /opt/conda/lib/python3.10/site-packages (3.6.0)\nRequirement already satisfied: umap-learn in /opt/conda/lib/python3.10/site-packages (0.5.6)\nRequirement already satisfied: click in /opt/conda/lib/python3.10/site-packages (from nltk) (8.1.7)\nRequirement already satisfied: joblib in /opt/conda/lib/python3.10/site-packages (from nltk) (1.4.2)\nRequirement already satisfied: regex>=2021.8.3 in /opt/conda/lib/python3.10/site-packages (from nltk) (2024.5.15)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from nltk) (4.66.4)\nRequirement already satisfied: numpy<2.0,>=1.18.5 in /opt/conda/lib/python3.10/site-packages (from gensim) (1.26.4)\nRequirement already satisfied: scipy<1.14.0,>=1.7.0 in /opt/conda/lib/python3.10/site-packages (from gensim) (1.13.1)\nRequirement already satisfied: smart-open>=1.8.1 in /opt/conda/lib/python3.10/site-packages (from gensim) (7.0.4)\nRequirement already satisfied: Jinja2>=2.9 in /opt/conda/lib/python3.10/site-packages (from bokeh) (3.1.4)\nRequirement already satisfied: contourpy>=1.2 in /opt/conda/lib/python3.10/site-packages (from bokeh) (1.2.1)\nRequirement already satisfied: packaging>=16.8 in /opt/conda/lib/python3.10/site-packages (from bokeh) (21.3)\nRequirement already satisfied: pandas>=1.2 in /opt/conda/lib/python3.10/site-packages (from bokeh) (2.2.3)\nRequirement already satisfied: pillow>=7.1.0 in /opt/conda/lib/python3.10/site-packages (from bokeh) (10.3.0)\nRequirement already satisfied: PyYAML>=3.10 in /opt/conda/lib/python3.10/site-packages (from bokeh) (6.0.2)\nRequirement already satisfied: tornado>=6.2 in /opt/conda/lib/python3.10/site-packages (from bokeh) (6.4.1)\nRequirement already satisfied: xyzservices>=2021.09.1 in /opt/conda/lib/python3.10/site-packages (from bokeh) (2024.9.0)\nRequirement already satisfied: scikit-learn>=0.22 in /opt/conda/lib/python3.10/site-packages (from umap-learn) (1.2.2)\nRequirement already satisfied: numba>=0.51.2 in /opt/conda/lib/python3.10/site-packages (from umap-learn) (0.60.0)\nRequirement already satisfied: pynndescent>=0.5 in /opt/conda/lib/python3.10/site-packages (from umap-learn) (0.5.13)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from Jinja2>=2.9->bokeh) (2.1.5)\nRequirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /opt/conda/lib/python3.10/site-packages (from numba>=0.51.2->umap-learn) (0.43.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=16.8->bokeh) (3.1.2)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas>=1.2->bokeh) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas>=1.2->bokeh) (2024.1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas>=1.2->bokeh) (2024.1)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.22->umap-learn) (3.5.0)\nRequirement already satisfied: wrapt in /opt/conda/lib/python3.10/site-packages (from smart-open>=1.8.1->gensim) (1.16.0)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas>=1.2->bokeh) (1.16.0)\n","output_type":"stream"}]},{"cell_type":"code","source":"import nltk\nnltk.download('wordnet')\nnltk.download('omw-1.4')\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.stem import PorterStemmer\nfrom nltk.tokenize import word_tokenize\n\nps = PorterStemmer()\nlemmatizer = WordNetLemmatizer()","metadata":{"execution":{"iopub.status.busy":"2024-10-06T14:15:50.056468Z","iopub.execute_input":"2024-10-06T14:15:50.056929Z","iopub.status.idle":"2024-10-06T14:15:50.312928Z","shell.execute_reply.started":"2024-10-06T14:15:50.056886Z","shell.execute_reply":"2024-10-06T14:15:50.311804Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stderr","text":"[nltk_data] Downloading package wordnet to /usr/share/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n[nltk_data] Downloading package omw-1.4 to /usr/share/nltk_data...\n","output_type":"stream"}]},{"cell_type":"code","source":"import pandas as pd\n# df = pd.read_csv('C:/Users/Valentina/Documents/Study/DS/lesson_22/labeled.csv')\ndf = pd.read_csv('/kaggle/input/labeled-1/labeled.csv.csv')","metadata":{"execution":{"iopub.status.busy":"2024-10-06T15:28:56.580954Z","iopub.execute_input":"2024-10-06T15:28:56.581334Z","iopub.status.idle":"2024-10-06T15:28:56.699144Z","shell.execute_reply.started":"2024-10-06T15:28:56.581299Z","shell.execute_reply":"2024-10-06T15:28:56.697963Z"},"trusted":true},"execution_count":116,"outputs":[]},{"cell_type":"code","source":"# data = list(open(\"./quora.txt/\", encoding=\"utf-8\"))\nprint(df['comment'][1])\n","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"MaFpN9pvxtNg","outputId":"d7aa8e42-ce7d-4f8e-ac33-4166a640c220","execution":{"iopub.status.busy":"2024-10-06T15:29:00.634812Z","iopub.execute_input":"2024-10-06T15:29:00.635231Z","iopub.status.idle":"2024-10-06T15:29:00.641585Z","shell.execute_reply.started":"2024-10-06T15:29:00.635182Z","shell.execute_reply":"2024-10-06T15:29:00.640269Z"},"trusted":true},"execution_count":117,"outputs":[{"name":"stdout","text":"Хохлы, это отдушина затюканого россиянина, мол, вон, а у хохлов еще хуже. Если бы хохлов не было, кисель их бы придумал.\n\n","output_type":"stream"}]},{"cell_type":"code","source":"tokenizer = WordPunctTokenizer()\n\nprint(tokenizer.tokenize(df['comment'][1]))","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HvXRbOKGx0l_","outputId":"f265a2aa-fc44-4763-bc87-fe05ee0a7203","execution":{"iopub.status.busy":"2024-10-06T14:10:51.454493Z","iopub.execute_input":"2024-10-06T14:10:51.454915Z","iopub.status.idle":"2024-10-06T14:10:51.461511Z","shell.execute_reply.started":"2024-10-06T14:10:51.454876Z","shell.execute_reply":"2024-10-06T14:10:51.460179Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"['Хохлы', ',', 'это', 'отдушина', 'затюканого', 'россиянина', ',', 'мол', ',', 'вон', ',', 'а', 'у', 'хохлов', 'еще', 'хуже', '.', 'Если', 'бы', 'хохлов', 'не', 'было', ',', 'кисель', 'их', 'бы', 'придумал', '.']\n","output_type":"stream"}]},{"cell_type":"code","source":"comments = []\nfor i in range(len(df['comment'])): \n      comments.extend(tokenizer.tokenize(df['comment'][i].lower()))\n#     comments=comment+(tokenizer.tokenize(df['comment'][i]))\n        \nprint(comments[:100])\n","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-10-06T14:29:57.730519Z","iopub.execute_input":"2024-10-06T14:29:57.730920Z","iopub.status.idle":"2024-10-06T14:29:58.109331Z","shell.execute_reply.started":"2024-10-06T14:29:57.730880Z","shell.execute_reply":"2024-10-06T14:29:58.107958Z"},"trusted":true},"execution_count":29,"outputs":[{"name":"stdout","text":"['верблюдов', '-', 'то', 'за', 'что', '?', 'дебилы', ',', 'бл', '...', 'хохлы', ',', 'это', 'отдушина', 'затюканого', 'россиянина', ',', 'мол', ',', 'вон', ',', 'а', 'у', 'хохлов', 'еще', 'хуже', '.', 'если', 'бы', 'хохлов', 'не', 'было', ',', 'кисель', 'их', 'бы', 'придумал', '.', 'собаке', '-', 'собачья', 'смерть', 'страницу', 'обнови', ',', 'дебил', '.', 'это', 'тоже', 'не', 'оскорбление', ',', 'а', 'доказанный', 'факт', '-', 'не', '-', 'дебил', 'про', 'себя', 'во', 'множественном', 'числе', 'писать', 'не', 'будет', '.', 'или', 'мы', 'в', 'тебя', 'верим', '-', 'это', 'ты', 'и', 'твои', 'воображаемые', 'друзья', '?', 'тебя', 'не', 'убедил', '6', '-', 'страничный', 'пдф', 'в', 'том', ',', 'что', 'скрипалей', 'отравила', 'россия', '?', 'анализировать', 'и', 'думать', 'пытаешься']\n","output_type":"stream"}]},{"cell_type":"code","source":"from nltk.corpus import stopwords\nstop_words= stopwords.words(\"russian\")\nstop_add=['.',',','-','(',')','?','это','!']\nstop_words.extend(stop_add)\nprint(stop_words)","metadata":{"execution":{"iopub.status.busy":"2024-10-06T14:45:46.009053Z","iopub.execute_input":"2024-10-06T14:45:46.009529Z","iopub.status.idle":"2024-10-06T14:45:46.018559Z","shell.execute_reply.started":"2024-10-06T14:45:46.009487Z","shell.execute_reply":"2024-10-06T14:45:46.017300Z"},"trusted":true},"execution_count":56,"outputs":[{"name":"stdout","text":"['и', 'в', 'во', 'не', 'что', 'он', 'на', 'я', 'с', 'со', 'как', 'а', 'то', 'все', 'она', 'так', 'его', 'но', 'да', 'ты', 'к', 'у', 'же', 'вы', 'за', 'бы', 'по', 'только', 'ее', 'мне', 'было', 'вот', 'от', 'меня', 'еще', 'нет', 'о', 'из', 'ему', 'теперь', 'когда', 'даже', 'ну', 'вдруг', 'ли', 'если', 'уже', 'или', 'ни', 'быть', 'был', 'него', 'до', 'вас', 'нибудь', 'опять', 'уж', 'вам', 'ведь', 'там', 'потом', 'себя', 'ничего', 'ей', 'может', 'они', 'тут', 'где', 'есть', 'надо', 'ней', 'для', 'мы', 'тебя', 'их', 'чем', 'была', 'сам', 'чтоб', 'без', 'будто', 'чего', 'раз', 'тоже', 'себе', 'под', 'будет', 'ж', 'тогда', 'кто', 'этот', 'того', 'потому', 'этого', 'какой', 'совсем', 'ним', 'здесь', 'этом', 'один', 'почти', 'мой', 'тем', 'чтобы', 'нее', 'сейчас', 'были', 'куда', 'зачем', 'всех', 'никогда', 'можно', 'при', 'наконец', 'два', 'об', 'другой', 'хоть', 'после', 'над', 'больше', 'тот', 'через', 'эти', 'нас', 'про', 'всего', 'них', 'какая', 'много', 'разве', 'три', 'эту', 'моя', 'впрочем', 'хорошо', 'свою', 'этой', 'перед', 'иногда', 'лучше', 'чуть', 'том', 'нельзя', 'такой', 'им', 'более', 'всегда', 'конечно', 'всю', 'между', '.', ',', '-', '(', ')', '?', 'это', '!']\n","output_type":"stream"}]},{"cell_type":"code","source":"filtered_comments=[]\nfor words in comments:\n    if words not in stop_words:\n        filtered_comments.append(words)\nprint(filtered_comments[:100])\nprint(len(filtered_comments))","metadata":{"execution":{"iopub.status.busy":"2024-10-06T14:55:55.111409Z","iopub.execute_input":"2024-10-06T14:55:55.112435Z","iopub.status.idle":"2024-10-06T14:55:56.013350Z","shell.execute_reply.started":"2024-10-06T14:55:55.112356Z","shell.execute_reply":"2024-10-06T14:55:56.012302Z"},"trusted":true},"execution_count":71,"outputs":[{"name":"stdout","text":"['верблюдов', 'дебилы', 'бл', '...', 'хохлы', 'отдушина', 'затюканого', 'россиянина', 'мол', 'вон', 'хохлов', 'хуже', 'хохлов', 'кисель', 'придумал', 'собаке', 'собачья', 'смерть', 'страницу', 'обнови', 'дебил', 'оскорбление', 'доказанный', 'факт', 'дебил', 'множественном', 'числе', 'писать', 'верим', 'твои', 'воображаемые', 'друзья', 'убедил', '6', 'страничный', 'пдф', 'скрипалей', 'отравила', 'россия', 'анализировать', 'думать', 'пытаешься', 'ватник', '?)', 'каких', 'стан', 'является', 'эталоном', 'современная', 'система', 'здравоохранения', 'рф', 'зимбабве', 'тупой', 'хохлы', 'шапке', 'ссылки', 'инфу', 'текущему', 'фильму', 'марвел', 'ссылки', 'заменены', 'фразу', 'репортим', 'брипидора', 'игнорируем', 'посты', 'недостаточно', 'понять', 'модератор', 'абсолютный', 'неадекват', 'нужно', 'лишить', 'полномочий', 'эта', 'борда', 'пробивает', 'абсолютное', 'дно', 'неадекватности', 'упад', 'т', 'строить', 'технологий', 'разворуют', 'трещинами', 'пош', 'л', 'тупые', 'китазы', 'могут', 'нормально', 'сделать', 'ебать', 'разносит', 'шизик', 'обосрался', 'сиди']\n252367\n","output_type":"stream"}]},{"cell_type":"code","source":"dictionary = {}\n\nfor i in range(len(filtered_comments)):\n    stem = lemmatizer.lemmatize(filtered_comments[i])\n    if stem in dictionary.keys():\n        dictionary[stem] = dictionary[stem] + 1\n    else: \n        dictionary[stem] = 1\ndf_lem = pd.DataFrame([dictionary]).T\n\ndf_lem.sort_values([0], ascending = False).head(10)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-06T15:28:44.613458Z","iopub.execute_input":"2024-10-06T15:28:44.613837Z","iopub.status.idle":"2024-10-06T15:28:47.141770Z","shell.execute_reply.started":"2024-10-06T15:28:44.613802Z","shell.execute_reply":"2024-10-06T15:28:47.140834Z"},"trusted":true},"execution_count":115,"outputs":[{"execution_count":115,"output_type":"execute_result","data":{"text/plain":"           0\nпросто  1082\n...     1057\n:        991\nочень    650\nвообще   648\nвсё      647\nлет      629\nещё      586\n2        524\n1        479","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>просто</th>\n      <td>1082</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>1057</td>\n    </tr>\n    <tr>\n      <th>:</th>\n      <td>991</td>\n    </tr>\n    <tr>\n      <th>очень</th>\n      <td>650</td>\n    </tr>\n    <tr>\n      <th>вообще</th>\n      <td>648</td>\n    </tr>\n    <tr>\n      <th>всё</th>\n      <td>647</td>\n    </tr>\n    <tr>\n      <th>лет</th>\n      <td>629</td>\n    </tr>\n    <tr>\n      <th>ещё</th>\n      <td>586</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>524</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>479</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"print(df.count())","metadata":{"execution":{"iopub.status.busy":"2024-10-06T14:53:37.123583Z","iopub.execute_input":"2024-10-06T14:53:37.123979Z","iopub.status.idle":"2024-10-06T14:53:37.131642Z","shell.execute_reply.started":"2024-10-06T14:53:37.123939Z","shell.execute_reply":"2024-10-06T14:53:37.130426Z"},"trusted":true},"execution_count":67,"outputs":[{"name":"stdout","text":"0    68443\ndtype: int64\n","output_type":"stream"}]},{"cell_type":"code","source":"from nltk.stem import SnowballStemmer\n\nsnowball = SnowballStemmer(language=\"russian\")\n\ndic_stem = {}\nfor i in range(len(filtered_comments)):\n    stem = snowball.stem(filtered_comments[i])\n    if stem in dic_stem.keys():\n        dic_stem[stem] = dic_stem[stem] + 1\n    else: \n        dic_stem[stem] = 1\n# print(dictionary)\ndf_stem = pd.DataFrame([dic_stem]).T\n\n# df_stem.sort_values([0], ascending = False).head(10)\nprint(df_stem.count())","metadata":{"execution":{"iopub.status.busy":"2024-10-06T14:53:21.743053Z","iopub.execute_input":"2024-10-06T14:53:21.743485Z","iopub.status.idle":"2024-10-06T14:53:36.850601Z","shell.execute_reply.started":"2024-10-06T14:53:21.743447Z","shell.execute_reply":"2024-10-06T14:53:36.849414Z"},"trusted":true},"execution_count":66,"outputs":[{"name":"stdout","text":"0    33595\ndtype: int64\n","output_type":"stream"}]},{"cell_type":"markdown","source":"###Задание 3: Подсчитайте количество разных слов до и после лемматизации","metadata":{"id":"a1SM3sn1zf1b"}},{"cell_type":"markdown","source":"###Задание 4: Подсчитайте количество разных слов до и после стемминга","metadata":{"id":"uxKa8yUUzqNN"}},{"cell_type":"code","source":"До стемминга и лемминга было 252367 слов\nПосле лемминга осталось 68443, после стемминга - 33595","metadata":{"id":"x91DX51qzszR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Bag Of Words (BOW)","metadata":{"id":"Csv2YN2IRXJB"}},{"cell_type":"code","source":"def vectorize(tokens):\n    ''' This function takes list of words in a sentence as input \n    and returns a vector of size of filtered_vocab.It puts 0 if the \n    word is not present in tokens and count of token if present.'''\n    vector=[]\n    for w in filtered_vocab:\n        vector.append(tokens.count(w))\n    return vector\ndef unique(sequence):\n    '''This functions returns a list in which the order remains \n    same and no item repeats.Using the set() function does not \n    preserve the original ordering,so i didnt use that instead'''\n    seen = set()\n    return [x for x in sequence if not (x in seen or seen.add(x))]\n\n#create a list of stopwords.You can import stopwords from nltk too\nstopwords=[\"to\",\"is\",\"a\"]\n\n#list of special characters.You can use regular expressions too\nspecial_char=[\",\",\":\",\" \",\";\",\".\",\"?\"]\n\nfor i in df['comment'][:10]:\n#Write the sentences in the corpus,in our case, just two \n    string1=i \n    string1=string1.lower()\n    tokens1=string1.split()\n\n    vocab=unique(tokens1)\n    filtered_vocab=[]\n    for w in vocab: \n        if w not in stopwords and w not in special_char: \n            filtered_vocab.append(w)\n    vector1=vectorize(tokens1)\n    print(tokens1)\n    print(vector1)\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"O8YWG3JhSFeZ","outputId":"325b4fa5-7d56-4993-b4d0-1aae78bad94e","execution":{"iopub.status.busy":"2024-10-06T15:36:26.404361Z","iopub.execute_input":"2024-10-06T15:36:26.405448Z","iopub.status.idle":"2024-10-06T15:36:26.415990Z","shell.execute_reply.started":"2024-10-06T15:36:26.405400Z","shell.execute_reply":"2024-10-06T15:36:26.414728Z"},"trusted":true},"execution_count":131,"outputs":[{"name":"stdout","text":"['верблюдов-то', 'за', 'что?', 'дебилы,', 'бл...']\n[1, 1, 1, 1, 1]\n['хохлы,', 'это', 'отдушина', 'затюканого', 'россиянина,', 'мол,', 'вон,', 'а', 'у', 'хохлов', 'еще', 'хуже.', 'если', 'бы', 'хохлов', 'не', 'было,', 'кисель', 'их', 'бы', 'придумал.']\n[1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 2, 1, 1, 1, 1, 1]\n['собаке', '-', 'собачья', 'смерть']\n[1, 1, 1, 1]\n['страницу', 'обнови,', 'дебил.', 'это', 'тоже', 'не', 'оскорбление,', 'а', 'доказанный', 'факт', '-', 'не-дебил', 'про', 'себя', 'во', 'множественном', 'числе', 'писать', 'не', 'будет.', 'или', 'мы', 'в', 'тебя', 'верим', '-', 'это', 'ты', 'и', 'твои', 'воображаемые', 'друзья?']\n[1, 1, 1, 2, 1, 2, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n['тебя', 'не', 'убедил', '6-страничный', 'пдф', 'в', 'том,', 'что', 'скрипалей', 'отравила', 'россия?', 'анализировать', 'и', 'думать', 'пытаешься?', 'ватник', 'что', 'ли?)']\n[1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n['для', 'каких', 'стан', 'является', 'эталоном', 'современная', 'система', 'здравоохранения', 'рф?', 'для', 'зимбабве?', 'ты', 'тупой?', 'хохлы']\n[2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n['в', 'шапке', 'были', 'ссылки', 'на', 'инфу', 'по', 'текущему', 'фильму', 'марвел.', 'эти', 'ссылки', 'были', 'заменены', 'на', 'фразу', 'репортим', 'брипидора,', 'игнорируем', 'его', 'посты.', 'если', 'этого', 'недостаточно,', 'чтобы', 'понять,', 'что', 'модератор', 'абсолютный', 'неадекват,', 'и', 'его', 'нужно', 'лишить', 'полномочий,', 'тогда', 'эта', 'борда', 'пробивает', 'абсолютное', 'дно', 'по', 'неадекватности.']\n[1, 1, 2, 2, 2, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n['упад', 'т!', 'там', 'нельзя', 'строить!', 'технологий', 'нет!', 'разворуют', 'как', 'всегда!', 'уже', 'трещинами', 'пош', 'л!', 'тупые', 'китазы', 'не', 'могут', 'ничего', 'нормально', 'сделать!']\n[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n['ебать', 'тебя', 'разносит,', 'шизик.']\n[1, 1, 1, 1]\n['обосрался,', 'сиди', 'обтекай']\n[1, 1, 1]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Задание 10: Реализовать Bag of words на data_tok (можно на NLTK, можно без)","metadata":{"id":"MOZ1qx05Q46b"}},{"cell_type":"code","source":"","metadata":{"id":"Tew2nQN4OCiW"},"execution_count":null,"outputs":[]}]}